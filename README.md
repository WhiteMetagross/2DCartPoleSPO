# 2D Cart Pole with Proximal Policy Optimization (PPO):

A reproducible, SB3-based implementation of PPO for discrete control on the Gymnasium CartPole environment (CartPole-v1). The project includes training, evaluation, visualization utilities, and Optuna hyperparameter tuning.

---

## Algorithm Overview: Proximal Policy Optimization (PPO):

PPO optimizes a stochastic policy by maximizing a clipped surrogate objective that constrains policy updates to remain close to the behavior policy. In this project, the PPO algorithm is provided by Stable-Baselines3 (SB3), and is configured through code in this repository.

Key elements (as implemented by SB3 and configured here):
- **Policy**: Stochastic policy for discrete actions with separate policy/value networks.
- **Advantage estimation**: Generalized Advantage Estimation (GAE).
- **Clipped objective**: Prevents overly large policy updates via ratio clipping.
- **Value function loss and entropy bonus**: Balances policy improvement, value accuracy, and exploration.

### Mathematical foundations (standard PPO):
- **Probability ratio**: $r_t(	heta) = \frac{\pi_	heta(a_t|s_t)}{\pi_{	heta_{old}}(a_t|s_t)}$
- **Clipped surrogate objective** (to be maximized):
  $$L^{CLIP}(	heta) = \hat{\mathbb{E}}_t[\min(r_t(	heta)\hat{A}_t, 	ext{clip}(r_t(	heta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]$$
- **Value loss** (to be minimized): $L^{VF}(	heta) = \hat{\mathbb{E}}_t[(V_	heta(s_t) - V_t^{target})^2]$
- **Entropy bonus**: $L^{ENT}(	heta) = \hat{\mathbb{E}}_t[H[\pi_	heta(\cdot|s_t)]]$
- **Final loss** (to be minimized): $L(	heta) = -L^{CLIP}(	heta) + c_1 L^{VF}(	heta) - c_2 L^{ENT}(	heta)$

---

## Implementation:

### Neural Network Architecture:
- **Policy network (pi)**: [128, 128] hidden units with Tanh activations.
- **Value network (vf)**: [128, 128] hidden units with Tanh activations.
- **Policy type**: `stable_baselines3` PPO with "MlpPolicy" and separate pi/vf nets (`net_arch`).

### Hyperparameters (from `config.py`):

| Name                  | Value        | Deprogramion                               |
|-----------------------|-------------:|-------------------------------------------|
| `learning_rate`       | 3e-4         | Adam learning rate                        |
| `n_steps`             | 1024         | Rollout steps per environment before an update |
| `batch_size`          | 256          | Minibatch size for SGD                    |
| `n_epochs`            | 10           | Number of epochs per update               |
| `gamma`               | 0.99         | Discount factor                           |
| `gae_lambda`          | 0.95         | GAE λ for bias-variance trade-off         |
| `clip_range`          | 0.2          | PPO ratio clipping ε                      |
| `ent_coef`            | 0.0          | Entropy coefficient c2                    |
| `vf_coef`             | 0.5          | Value loss coefficient c1                 |
| `max_grad_norm`       | 0.5          | Global gradient clipping                  |
| `policy_hidden_dims`  | (128, 128)   | Pi & Vf MLP hidden sizes                  |
| `seed`                | 17           | Random seed                               |
| `device`              | cpu          | Device for training                       |

Additional configuration:
- **`ENV_NAME`**: "CartPole-v1"
- **`N_ENVS`**: 8
- **`TOTAL_TIMESTEPS`**: 200,000
- **`EVAL_INTERVAL`**: 2 updates
- **`TARGET_REWARD`**: 500.0

### Training Procedure:
- Parallel rollout collection using 8 `SubprocVecEnv` workers.
- Each update uses 1024 steps per environment with minibatch SGD (`batch_size=256`) for 10 epochs.
- Periodic evaluation on a separate environment.
- Automatic best model saving, and final model saved as `checkpoints/final_model.zip`.

---

## Project Structure:
```
2DCartPolePPO/
├─ config.py                      #Central configuration (env and PPO hyperparameters).
├─ train.py                       #Training entry point (SB3 PPO).
├─ evaluate.py                    #Model evaluation program.
├─ hyperparameter_optimization.py #Optuna-based hyperparameter tuning.
├─ visualization.py               #Log parsing and plotting utilities.
├─ checkpoints/                   #Saved models and best checkpoints.
├─ logs/                          #Training logs and evaluation artifacts.
├─ results/                       #Exported plots generated by visualization tools.
└─ visuals/                       #Static explanatory visuals for the README.
```
### Data flow:
1. `train.py` reads `Config`, creates vectorized envs, and trains PPO (SB3), logging to `logs/` and saving to `checkpoints/`.
2. `evaluate.py` loads a saved model and computes episode returns, saving plots to `results/`.
3. `visualization.py` parses log files in `logs/` and exports figures.
4. `hyperparameter_optimization.py` runs Optuna trials to select PPO hyperparameters.

---

## Dependencies and Setup:

Tested with Python 3.11 on Windows 11. Recommended package set:
- `stable-baselines3`
- `gymnasium`
- `torch`
- `pandas`, `seaborn`, `matplotlib`
- `optuna`

Install with pip:
```bash
pip install stable-baselines3 gymnasium[all] torch pandas seaborn matplotlib optuna
```
---

## Usage Guide:

### Train:
```bash
python train.py
```
**Behavior**:
- Creates 8 parallel envs and trains for 200,000 timesteps.
- Saves best model under `checkpoints/best_model.zip` and final model at `checkpoints/final_model.zip`.
- Early stops when evaluation reward ≥ 500.

### Evaluate:
```bash
python evaluate.py
```
**Behavior**:
- Loads the best model from `checkpoints/`.
- Runs 100 evaluation episodes.
- Saves analysis plots to `evaluation_plots.png` and `evaluation_analysis.png`.

### Visualize Training Logs:
```bash
python visualization.py
```
**Behavior**:
- Parses `logs/training_history.json`.
- Generates and saves `ppo_training_progress.png` and `ppo_reward_analysis.png`.

### Hyperparameter Tuning (Optuna):
```bash
python hyperparameter_optimization.py
```
**Behavior**:
- Runs Optuna trials to find optimal hyperparameters.
- Saves the best set of parameters to `best_hyperparameters.json`.

---

## Results and Analysis:

All referenced paths are relative to the repository root.

### Training Output:
The training program provides real time feedback on the agent's performance. The following is an example of the output during training, showing key metrics like reward, policy loss (PL), value loss (VL), and learning rate (LR).

Based on the training output, the agent shows rapid learning. After just 6 updates, it achieves an average reward of 235.10. Performance quickly jumps to 448.30 by the 8th update. The agent reaches the maximum possible reward of 500.00 by the 10th update, after a total of 81,920 timesteps, at which point the training successfully concludes.

![Training Information](visuals/PPOTrainingInfoOutput.png)

### Training Progress:
The `ppo_training_progress.png` plot shows the learning dynamics over the course of training.

![Training Progress](visuals/ppo_training_progress.png)

### Reward Analysis:
The `ppo_reward_analysis.png` plot provides a deeper look into the reward distribution and learning curve.

![Reward Analysis](visuals/ppo_reward_analysis.png)

### Evaluation Plots:
After training, the `evaluate.py` program generates detailed plots of the agent's performance over 100 episodes.

![Evaluation Plots](visuals/evaluation_plots.png)

![Evaluation Analysis](visuals/evaluation_analysis.png)

---

## Specifications: Environment:

**Environment**: Gymnasium `CartPole-v1`.

- **Observation space**: `Box(4,)`:
  1. Cart Position.
  2. Cart Velocity.
  3. Pole Angle.
  4. Pole Angular Velocity.
- **Action space**: `Discrete(2)`:
  - 0: Push cart to the left.
  - 1: Push cart to the right.
- **Reward**: +1 for every step taken.
- **Termination conditions**:
  - Pole Angle is more than 12 degrees from vertical.
  - Cart Position is more than 2.4 units from the center.
  - Episode length is greater than 500.

---

## Modifying Configurations:

Edit `config.py` to change:
- Environment (`env_name`, `num_envs`).
- Timesteps and evaluation frequency.
- PPO hyperparameters.
- Logging directories and model save paths.

---

## Reproducibility:
- **Fixed random seed**: 17 (in `config.py`).
- **Deterministic evaluation.**
- Clear logging and artifact paths for experiments (`checkpoints/`, `logs/`, `results/`).

---

## Citation and Acknowledgments
- Algorithm and core implementation provided by Stable-Baselines3 PPO.
- Environment provided by Gymnasium.
