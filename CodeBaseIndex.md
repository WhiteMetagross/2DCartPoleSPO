# CartPole-v1 SPO Project: Codebase Index

## Project Overview:
This repository uses the SPOinPyTorch library's SPO (Simple Policy Optimizations) implementation for solving the Gymnasium CartPole-v1 environment (discrete control). The codebase includes:
- Centralized configuration (single source of truth).
- Training, evaluation, and visualization utilities.
- Optuna based hyperparameter tuning.
- Checkpoints with embedded configuration for reproducibility.

---

## Index by Category:

### Configuration:
- **`config.py`**
  - **Purpose**: Provides a centralized configuration object with Optuna optimized hyperparameters.
  - **Key Function**: `get_default_config()` returns a `Config` object containing all necessary parameters for training and evaluation.
  - **Notes**: This file acts as the single source of truth for hyperparameters. The training script saves this configuration into every checkpoint, and the evaluation script loads it to ensure reproducibility.

### Training:
- **`train.py`**
  - **Purpose**: The main entry point for training the SPO agent using vectorized environments.
  - **Highlights**:
    - Uses `get_default_config()` for all hyperparameters.
    - Performs periodic evaluation every `eval_interval` updates.
    - Implements early stopping based on `target_reward` and `early_stopping_patience`.
    - Saves `checkpoints/best_model.pth` and `checkpoints/final_model.pth`, embedding the configuration within them.
    - Logs detailed metrics to `logs/training_log.txt` and `logs/training_history.json`.

### Evaluation:
- **`evaluate.py`**
  - **Purpose**: Loads a trained agent and computes its performance over 100 episodes, producing plots and a summary JSON file.
  - **Highlights**:
    - Automatically loads the configuration from the model checkpoint to prevent architecture or hyperparameter mismatches.
    - Generates and saves `visuals/evaluation_plots.png` and `visuals/evaluation_analysis.png`.
    - Saves a summary of metrics to `results/evaluation_results.json`.

### Hyperparameter Optimization:
- **`hyperparameter_optimization.py`**
  - **Purpose**: Uses the Optuna library to perform an automated search for the best SPO hyperparameters.
  - **Highlights**:
    - Implements a search space over network architecture, learning rates, and algorithm-specific parameters like `epsilon`.
    - Uses Optuna's `MedianPruner` to terminate underperforming trials early.
    - Saves the best discovered parameters to `results/best_hyperparameters.json`.

### Visualization:
- **`visualization.py`**
  - **Purpose**: Parses training logs to produce figures that visualize the learning process.
  - **Highlights**:
    - Can load data from either `logs/training_history.json` or `logs/training_log.txt`.
    - Generates `visuals/spo_training_progress.png` (showing reward and loss curves) and `visuals/spo_reward_analysis.png` (showing reward distribution and stability).

---

## Checkpoints and Reproducibility:
- Every saved checkpoint (`.pth` file) includes:
  - `actor_state_dict`, `critic_state_dict`, `optimizer_state_dict`
  - The update index and recent evaluation metrics.
  - **`config`**: A full dictionary copy of the configuration used for training that agent.
- The evaluation script (`evaluate.py`) automatically loads and applies this saved config, ensuring that the model architecture and hyperparameters match the training run exactly.

---

## Directory Layout:
```
2DCartPoleSPO/
├─ config.py                      #Central configuration (single source of truth).
├─ train.py                       #SPO training entry point.
├─ evaluate.py                    #Evaluation with plots, and loads config from checkpoint.
├─ hyperparameter_optimization.py #Optuna based tuning.
├─ visualization.py               #Plot training progress and reward analysis.
├─ checkpoints/                   #Saved models (best/final) with embedded config.
├─ documents/                     #Markdown documentation and papers.
├─ logs/                          #Training logs (JSON + text).
├─ results/                       #JSON outputs from evaluation and optimization.
└─ visuals/                       #PNG images generated by scripts.
```

---

## Centralized Configuration:
Key values currently in `get_default_config()` (derived from `results/best_hyperparameters.json`):
- **Network**: `actor_hidden_dims` and `critic_hidden_dims` = `[128, 128, 128, 128]`
- **Optimization**: `steps_per_batch` = 2048, `update_epochs` = 10, `num_minibatches` = 64
- **Learning**: `learning_rate` ≈ 7.28e-4, `gamma` ≈ 0.992, `gae_lambda` ≈ 0.952, `epsilon` (SPO) ≈ 0.299
- **Regularization**: `entropy_coeff` ≈ 0.0044, `value_loss_coeff` ≈ 0.507, `max_grad_norm` ≈ 1.68
- **Evaluation/Stopping**: `eval_interval` = 2, `target_reward` = 500.0, `early_stopping_patience` = 20

---

## Documentation Files:
- **`README.md`**: Project overview, SPO and PPO comparisons based on th results of this project, and visual results.
- **`codebase_index.md`**: This file.
- **`documents/`**: Contains the original SPO research paper and other supplementary documents.
- **`visuals/`**: Contains all plots and images referenced by the documentation.

---

## References:
- SPO: Simple Policy Optimization (arXiv:2401.16025)
- Gymnasium CartPole-v1
- Optuna
- PyTorch

